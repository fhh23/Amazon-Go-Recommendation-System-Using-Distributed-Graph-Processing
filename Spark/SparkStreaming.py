from __future__ import print_function

import os
import pyspark
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils
from tdigest import TDigest
import pprint
from pyspark import SparkContext, SparkConf
from operator import add
import Itemsets

percentile_broadcast = None

def digest_partitions(values):
    digest = TDigest()
    digest.batch_update(values)
    return [digest] 

def compute_percentile(rdd):
    global percentile_broadcast
    percentile_limit = rdd.map(lambda row: int(row[1])) \
                          .mapPartitions(digest_partitions) \
                          .reduce(add) \
                          .percentile(50)
    percentile_broadcast = rdd.context.broadcast(percentile_limit)

def filter_most_popular(rdd):
    global percentile_broadcast
    if percentile_broadcast:
        return rdd.filter(lambda row: row[1] > percentile_broadcast.value)
    return rdd.context.parallelize([])

sc = SparkContext(appName='streamingFromKafka')
ssc = StreamingContext(sc, 15)
# Set the Kafka topic
topic = 'fh-topic'

# List the Kafka Brokers
broker_file = open('brokers.txt', 'r')
kafka_brokers = broker_file.read()[:-1]
broker_file.close()
kafkaBrokers = {"metadata.broker.list": "ec2-35-166-31-140.us-west-2.compute.amazonaws.com:9092"}

# Create input stream that pull messages from Kafka Brokers (DStream object)
trans = KafkaUtils.createDirectStream(ssc, [topic], kafkaBrokers)
body = trans.map(lambda x: x[1])#.foreachRDD(lambda RDD: print(RDD.collect()))
lines = body.flatMap(lambda bodys: bodys.split("\r\n"))#.foreachRDD(lambda RDD: print(RDD.collect())) 
'''
word = lines.map(Itemsets.lineSplit) \
            .map(lambda word: (word, 1)) \
            .reduceByKey(lambda a, b: a+b)
            #.map(lambda x:x[1]) 
print_word = word.pprint()
print("RDD filtered: \n") 
word.foreachRDD(compute_percentile)
filter_digest = word.transform(filter_most_popular).foreachRDD(lambda RDD: print(RDD.collect()))
'''



numPartitions = 1 
s = .3
#count = lines.count()
count = 10
threshold = s*count
#split string baskets into lists of items
baskets = lines.map(Itemsets.lineSplit2) \
               .map(lambda (a,b): (int(a), int(b))) \
               .groupByKey() \
               .mapValues(list) \
               .map(lambda x: sorted(x[1])).cache()
print("RDD: \n")
#b2 = baskets.foreachRDD(lambda RDD: print(RDD.collect()))

#treat a basket as a set for fast check if candidate belongs
basketSets = baskets.map(set).cache()
#each worker calculates the itemsets of his partition
localItemSets = baskets.mapPartitions(lambda data: [x for y in Itemsets.get_frequent_items_sets(data, threshold/numPartitions).values() for x in y], True)

#for reducing by key later
allItemSets = localItemSets.map(lambda n_itemset: (n_itemset,1))
#merge candidates that are equal, but generated by different workers
mergedCandidates = allItemSets.reduceByKey(lambda x,y: x).map(lambda (x,y): x)
#distribute global candidates to all workers

mergedC = mergedCandidates.foreachRDD(lambda RDD: print(RDD.collect()))
'''
candidates = sc.broadcast(mergedCandidates)

#count actual occurrence of candidates in document
counts = baskets.map(set).flatMap(lambda line: [(candidate,1) for candidate in candidates.value if line.issuperset(candidate)])    
#counts = basketSets.flatMap(lambda line: [(candidate,1) for candidate in candidates.value if line.issuperset(candidate)])
#filter finalists
finalItemSets = counts.reduceByKey(lambda v1, v2: v1+v2).filter(lambda (i,v): v>=threshold)
#put into nice format
print("\n Itemsets filtered: \n")
finalItemSets = finalItemSets.map(lambda (itemset, count): ", ".join([str(x) for x in itemset])+"\t("+str(count)+")")

'''
#f = open('digest.txt', 'a')
#print(" OKOKOK ", file=f)
#print(digest.percentile(50), file=f)
#print("\n", file=f)
#f.close()
ssc.start()
ssc.awaitTermination()

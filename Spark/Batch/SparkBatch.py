from __future__ import print_function

import os
import pyspark
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils
import pprint
from pyspark import SparkContext, SparkConf
from operator import add
import sys
import boto

percentile_broadcast = None

def splitFunc(rdd):
    import Itemsets
    Itemsets.lineSplit(rdd)
	
def splitFunc2(rdd):
    import Itemsets
    Itemsets.lineSplit2(rdd)
	
#======================================Apriori algo===================================================
def get_frequent_items_sets(data,min_sup,steps=0):

	# we transform the dataset in a list of sets
	transactions = list()

	# Temporary dictionary to count occurrences
	items = defaultdict(lambda: 0)

	# Returned dictionary
	solution = dict()
	L_set = set()

	# Fills transactions and counts "singletons"
	for line in data:
		transaction = set(line)
		transactions.append(transaction)
		for element in transaction:
			items[element]+=1

	# Add to the solution all frequent items
	for item, count in items.iteritems():
		if count >= min_sup:
			L_set.add(frozenset([item]))

	# Generalize the steps it ends when there are no candidates
	# or if the user provided as input the number of parameters
	k = 0
	solution[k-1] = L_set
	while L_set != set([]) and k != steps+1:
		L_set = create_candidates(L_set,k)
		C_set = frequent_items(L_set,transactions,min_sup)
		if C_set != set([]): solution[k]=C_set
		L_set = C_set
		k = k + 1
	return solution





# Creates candidates joining the same set
# input:
#    itemSet: a set of frequent items
#     length: the cardinality of generated combinations
#  returns:
#	   set: a set containing all combinations with cardinality equal to length
#		(be carefull on iterating on it)

def create_candidates(itemSet, length):
        return set([i.union(j) for i in itemSet for j in itemSet if len(i.union(j)) == length])




# Checks occurrences of items in transactions and returns frequent items
# input:
#    items: a set of frequent items (candidates)
#     transactions: list of sets representing baskets
#  returns:
#	   _itemSet: a set containing all frequent candidates (a subset of inputs)

def frequent_items(items, transactions, min_sup):
	_itemSet = set()
	counter = defaultdict(int)
	localDict = defaultdict(int)
	for item in items:
		for transaction in transactions:
			if item.issubset(transaction):localDict[item] += 1

	for item, count in localDict.items():
		if count >= min_sup:
			_itemSet.add(item)
	return _itemSet


# helper to standardize the output (not part of the algorithm)

def please_clean(solution):
	res = []
	for i in solution.itervalues():
		for j in i:
			res.append(j)
	return res

#======================================================================================================

sc = SparkContext(appName='ItemsetsBatch')

# Read in from S3
aws_access_key = os.getenv('AWS_ACCESS_KEY_ID', 'default')
aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY', 'default')

conn = boto.connect_s3(aws_access_key, aws_secret_access_key)
bucket = conn.get_bucket('fh-data-insight')
#data = sc.testFile("s3n://fh-data-insight/*")
#print(data.take(5)

bucket_path = "s3n://fh-data-insight/"
for f in bucket:
    data = sc.textFile(bucket_path + f.name)
	body = data.map(lambda x: x[1])
	lines = body.flatMap(lambda bodys: bodys.split("\r\n")) 
	word = lines.map(splitFunc) \
				.map(lambda word: (word, 1)) \
				.reduceByKey(lambda a, b: a+b)
				
	print(word.take(5))
	numPartitions = data.getNumPartitions() 
	s = .3
	count = lines.count()
	#count = 10
	threshold = s*count
	#split string baskets into lists of items
	baskets = lines.map(splitFunc2) \
				   .map(lambda (a,b): (int(a), int(b))) \
				   .groupByKey() \
				   .mapValues(list) \
				   .map(lambda x: sorted(x[1]))



	#treat a basket as a set for fast check if candidate belongs
	basketSets = baskets.map(set).persist()
	#each worker calculates the itemsets of his partition
	localItemSets = baskets.mapPartitions(lambda data: [x for y in get_frequent_items_sets(data, threshold/numPartitions).values() for x in y], True)

	#for reducing by key later
	allItemSets = localItemSets.map(lambda n_itemset: (n_itemset,1)) 
	#merge candidates that are equal, but generated by different workers


	mergedCandidates = allItemSets.reduceByKey(lambda x,y: x).map(lambda (x,y): x).filter(lambda r: len(r) > 0) 

	mergedCandidates = mergedCandidates.collect()

	candidates = sc.broadcast(mergedCandidates)

	#count actual occurrence of candidates in document   
	counts = basketSets.flatMap(lambda line: [(candidate,1) for candidate in candidates.value if line.issuperset(candidate)])
	#filter finalists

	finalItemSets = counts.reduceByKey(lambda v1, v2: v1+v2).filter(lambda (i,v): v>=threshold)
	#put into nice format
	finalItemSets = finalItemSets.map(lambda (itemset, count): ", ".join([str(x) for x in itemset])+"\t("+str(count)+")")
	finalItemSets.saveAsTextFile("spark_out.txt")
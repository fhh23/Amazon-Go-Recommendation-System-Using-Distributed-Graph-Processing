from __future__ import print_function

import os
import pyspark
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils
from tdigest import TDigest
import pprint
from pyspark import SparkContext, SparkConf
from operator import add
import Itemsets
import time
from pyspark.accumulators import AccumulatorParam

percentile_broadcast = None

class ListParam(AccumulatorParam):
    def zero(self, v):
        return []
    def addInPlace(self, acc1, acc2):
        acc1.extend(acc2)
        return acc1

def file_read1(rdd):
    global list2 # Required otherwise the next line will fail
    list2 += [rdd.collect()]
    #print(list2.value)
    return rdd

def digest_partitions(values):
    digest = TDigest()
    digest.batch_update(values)
    return [digest] 

def compute_percentile(rdd):
    global percentile_broadcast
    percentile_limit = rdd.map(lambda row: int(row[1])) \
                          .mapPartitions(digest_partitions) \
                          .reduce(add) \
                          .percentile(50)
    percentile_broadcast = rdd.context.broadcast(percentile_limit)

def filter_most_popular(rdd):
    global percentile_broadcast
    if percentile_broadcast:
        return rdd.filter(lambda row: row[1] > percentile_broadcast.value)
    return rdd.context.parallelize([])

def MyCollect(rdd, result):
    global list1
    if rdd:
        r = rdd.collect()
        if r:
            list1.append(r)
            print(result)

sc = SparkContext(appName='streamingFromKafka')
ssc = StreamingContext(sc, 5)
# Set the Kafka topic
topic = 'fh-topic'

# List the Kafka Brokers
broker_file = open('brokers.txt', 'r')
kafka_brokers = broker_file.read()[:-1]
broker_file.close()
kafkaBrokers = {"metadata.broker.list": "ec2-35-166-31-140.us-west-2.compute.amazonaws.com:9092"}

# Create input stream that pull messages from Kafka Brokers (DStream object)
trans = KafkaUtils.createDirectStream(ssc, [topic], kafkaBrokers)
body = trans.map(lambda x: x[1])#.foreachRDD(lambda RDD: print(RDD.collect()))
lines = body.flatMap(lambda bodys: bodys.split("\r\n"))#.foreachRDD(lambda RDD: print(RDD.collect())) 
'''
word = lines.map(Itemsets.lineSplit) \
            .map(lambda word: (word, 1)) \
            .reduceByKey(lambda a, b: a+b)
            #.map(lambda x:x[1]) 
print_word = word.pprint()
print("RDD filtered: \n") 
word.foreachRDD(compute_percentile)
filter_digest = word.transform(filter_most_popular).foreachRDD(lambda RDD: print(RDD.collect()))
'''



numPartitions = 1 
s = .3
#count = lines.count()
count = 10
threshold = 2#s*count
#split string baskets into lists of items
baskets = lines.map(Itemsets.lineSplit2) \
               .map(lambda (a,b): (int(a), int(b))) \
               .groupByKey() \
               .mapValues(list) \
               .map(lambda x: sorted(x[1])).cache()
print("RDD: \n")
#b2 = baskets.foreachRDD(lambda RDD: print(RDD.collect()))

#treat a basket as a set for fast check if candidate belongs
basketSets = baskets.map(set).cache()
#each worker calculates the itemsets of his partition
localItemSets = baskets.mapPartitions(lambda data: [x for y in Itemsets.get_frequent_items_sets(data, threshold/numPartitions).values() for x in y], True)

#for reducing by key later
allItemSets = localItemSets.map(lambda n_itemset: (n_itemset,1))
print("\n \n MERGED CAND \n")
#merge candidates that are equal, but generated by different workers
mergedCandidates = allItemSets.reduceByKey(lambda x,y: x).map(lambda (x,y): x).filter(lambda r: len(r) > 0)

list2 = mergedCandidates.context().sparkContext.accumulator([], ListParam())
#rdd = sc.parallelize(range(10)).map(file_read1).collect()
mergedCandidates.map(file_read1).foreachRDD(lambda rdd: rdd.collect())
print(list2.value)
#distribute global candidates to all workers
#candidates = sc.broadcast(mergedC)
'''
f = open("lists.txt", 'r+')
mergedCandidates.filter(lambda r: len(r) > 0).foreachRDD(lambda rdd: print(rdd.collect(), f))
print("NEXT \n", f)
data = f.read().splitlines()#[line.strip() for line in f]
f.truncate()
f.seek(0)
print(data)
print("\n NEXT \n")
candidates = sc.broadcast(data)
'''
#mergedCandidates = mergedCandidates.filter(lambda r: len(r) > 0)
#count actual occurrence of candidates in document
#counts = mergedCandidates.filter(lambda r: len(r) > 0).flatMap(lambda line: (line,1)).foreachRDD(lambda rdd: print(rdd.collect()))    
#counts = basketSets.flatMap(lambda line: [(candidate,1) for candidate in candidates.value if line.issuperset(candidate)])
#filter finalists
'''
finalItemSets = counts.reduceByKey(lambda v1, v2: v1+v2).filter(lambda (i,v): v>=threshold)
#put into nice format
finalItemSets = finalItemSets.map(lambda (itemset, count): ", ".join([str(x) for x in itemset])+"\t("+str(count)+")")
finalItemSets.saveAsTextFiles("spark_out.txt")
#f = open('digest.txt', 'a')
#print(digest.percentile(50), file=f)
#print("\n", file=f)
'''
ssc.start()
ssc.awaitTermination()
